## 快手

##### 1 你是怎么对数仓分层的？

1）ODS => 元数据层

2）EDW=>DWB 原子数据层的快照，将ETL转化为 EL + T

3）DWD => 面向业务过程的明细数据层（最细粒度的数据分析）

4）DW => 事件域 或 主题的轻度汇总层（用于面向主题或事件流转的分析，方便查询和使用）

5）DM => 数据集市层，主要是面向业务需求的分析主题的高度汇总表

##### 2 你是怎么建立DWD的？步骤是什么？

- 分析需求
- 分析数据
- 形成 ER 图

1）维表

- 根据业务需求确定维度
- 确定维度属性（丰富化维表，可以冗余以确保维度能够完整的描述业务过程或者事实表的环境，如日期、产品、资方等）
- 确定维度层级关系：分析维度层级关系以更好的理解维度关系以及业务流程关系；对于多层级维度
  - 一对多且层级固定可以做退化维度，比如日期维表可以将月和年退化到日期维表中
  - 多对多的情况，可以考虑做桥接表或者作为支架表方便使用
- 处理缓慢变化维：针对需求是否对维度变化历史有分析需要进行
  - 新增列的方式 - 适用于周期性变化维度，且只关心状态变化
  - 新增行的方式 - 适用于多个维度属性的变化，一般而言都是周期性的拉链表实现方式
  - 垂直拆分 - 对变化的维度属性和非变化的维度属性进行拆分，非变化为主表，从表可以采用新增列或新增行的方式进行缓慢变化维的处理。

2）事实表

- 根据业务需求进行业务过程的选择
- 按需求确定事实表类型：DWD层普遍都是事务事实表
- 声明粒度：DWD普遍都是原子数据粒度（必须做粒度保障）
- 确定维度：事实表的维度属性（对于杂项维度，可以进行统一存储和加工；对于其他维度属性尽可能的描述事实环境并满足业务分析需求）
- 确定事实：事实表的度量，也就是通过维度进行分析的度量值，一般分为：可加、不可加、半可加



- 基于上诉步骤，可以形成自己的建模文档：包括ER图，业务流程图、总线矩阵的维护、物理表的设计、ETL的设计
- 模型 + ETL 评审
- 自测 + 交叉测
- 数据准备
- 上线

##### 3 你都了解什么大数据技术栈?

Hive、SPARK、MR、HDFS、Kafka、Hbase、Flink、Kylin

##### 4 你的工作是偏业务更多还是偏向编程更多？

偏向业务，也就是偏建模和SQL

##### 5 你对Hive了解多少？

1）可以将 HIVE 简单理解为数据库，但是和数据库又有些差别，HIVE是一个帮助我们对 HDFS 上的数据进行存储和读取的工具；提供了对数据的表格化抽象以及 SQL 语言来简化读写

2）通常我们都利用 HIVE 来进行数仓的数据定义以及数据管理

##### 6 Hive的底层是什么？

1）HDFS => 是数据真正存储文件系统

2）MR、SPARK、TEZ等 => 是HIVE可利用的计算引擎

3）SERVER => 负责对 SQL进行词法分析、语法分析、执行计划的生成并针对不同的计算引擎进行物理执行计划的转化

4）元数据服务 => 是HIVE提供元数据服务的组件、通常默认使用 derby 存储元数据，生产环境中一般采用 MySQL 进行元数据的存取。

##### 7 一个SQL是怎么翻译成MapReduce  job的？

1）词法分析

2）语法分析

3）从元数据服务获取元数据，判定表、字段的存在与否

4）根据语法树转化操作树

5）将操作树转化为逻辑执行计划

6）将逻辑执行计划转化为物理执行计划

5）优化器进行优化

##### 8 你知道数据倾斜吗？

通常我们认为在分布式计算中，并行处理的数据之间存在数据量较大的差异，导致部分并行任务执行时间过长，就是数据倾斜。

1）MAP端：参与计算的 HIVE 表小文件多，且存在较大的文件，也算数据倾斜（但是由于 splitSize 的限制，通常而言这种倾斜感知不大）

2）Reduce 端：MAP端产生的数据按照 KEY进行划分并进行分区，分区数量取决于 参数指定的reduce task 个数或动态计算（map端总size / reduce per size；小于1时为1；需要全局排序或者去重时都是1），当 某些 key 数据量大时会出现数据倾斜，导致 reduce 的部分 task 执行时间远高于其他 task

3）SPARK 中数据倾斜往往可能造成 OOM

##### 9 现在有很长一段SQL，其中第一段是5小段union all在一起的，这5小段都进行了group by，现在产生了数据倾斜，怎么确定到底是哪一段SQL代码产生的数据倾斜？

1）首先判断数据倾斜不可能来自 union all 操作，因为此操作不涉及 key的划分

2）通过 explain 可以查看 stage 划分，以及各种 operator 输入输出的数据量，但是explain 并不能查看到 task 级别的数据量，所以不能区分出是否发生了数据倾斜，但是根据执行计划可以分析出哪些stage 发生了 shuffle

3）可以通过YARN 上查看作业信息来判断task的情况，定位到 stage 及 operator

4）定位到具体 operator 之后就可以根据 explain的信息定位到哪段SQL了

##### 10 在group by的时候产生了数据倾斜该怎么办？在join的时候产生了数据倾斜该怎么办？

1）GROUP BY 

- 可以考虑开启 map agg 来进行map端的agg（map 端的 agg 需要保证咱们的聚合运算满足预聚合值的可加性）
- 可以考虑进行倾斜key 的随机数打散操作，再进行合并计算

2）JOIN

- 如果是小表JOIN大表 - 考虑进行 map join 操作（具体满足map join 的数据的大小看配置参数）；SparkSQL 中可以用（/*+ MAPJOIN(小表) */）或者推荐使用的 （/*+ BROADCAST(小表) */）

- 不满足 map join 条件

  - 倾斜key 为空 - 随机数打散 => concat('kkk11', cast(rand() * 10 as int))
  - 倾斜key不为空 - 需要将两表中倾斜数据的key筛选出来，非倾斜表进行膨胀（可以关联维表实现或则利用 space函数加 explode函数来实现）；倾斜表需要进行随机，随机的范围应该为非倾斜表的膨胀范围；然后进行关联并且与非倾斜部分数据的关联结果进行合并

  ```sql
  -- 主播直播间关系表
  with employee_live as (select employee_id, live_id from employee_live)
  -- 直播间弹幕表（某些直播间人气高，弹幕远远大于其它直播间）
  , live_log as (select live_id, user_id, text from live_log)
  -- 倾斜处理
  , elt as (
  	select elt.employee_id, concat(elt.live_id, xtb.index) as new_live_id -- 0 ~ 9
      from (
          select employee_id, live_id from employee_live 
          where live_id in (倾斜key)
      ) as elt
      join (
          select explode(split(space(9), ' ')) xtb as index, ele
      ) as xtb on 1 = 1
  )
  , llt as (
  	select live_id, user_id, text, concat(live_id, cast(rand() * 10 as int)) as new_live_id -- 0 ~ 9
      from live_log
      where live_id in (倾斜key)
  )
  .... 
  ```

- 可以将上诉倾斜部分的逻辑抽离出来作为中间表实现，另外可以先对倾斜表的 key分布情况进行一个统计再进行筛选，避免后期不断地维护指定的倾斜 key

##### 11 一个用户行为数据表，表结构为 userid和date ，存放用户的登录数据，登录了则记录登录的日期，当天没登录则不记录，比如 001用户20190101登录则记录 001 20190101，现在要求10月连续登录三天的用户（一个月只算一次，比如当月连续登录了4天，也只算一次连续登录三天），输出 用户id和是否连续登录

 ```sql
-- user_action 表 => user_id, date
-- 连续登录的经典题，分析结果输出，十月份：用户ID，是否连续登录
select user_id, max(if(lx_cnt >= 3, 1, 0)) as is_3_day
from (
    select user_id, login_start_date, count(1) as lx_cnt
    from (
        select
            user_id, login_date, date_sub(login_date, rn - 1) as login_start_date
        from (
            select 
                user_id,
                date_parse(`date`, 'yyyyMMdd') as login_date,
                row_number() over(partition by user_id order by `date` asc) as rn
            from user_action
            where substr(`date`, 1, 6) = '2019-10'
        ) as tmp
    ) as tmp
    group by user_id, login_start_date
) as tmp
group by user_id
 ```



## 美团

##### 1 你们的数仓分层是什么？



##### 2 你是怎么看待这个分层的？

1）首先能够满足业务对各类数据的分析需求（明细+汇总）

2）扩展性强，层次内可进行自由扩展，层次间能进行逻辑下沉以减少重复逻辑开发

3）响应变化的能力，针对业务、需求、数据的变化，一般只需在DWD层进行处理即可完成整体的修改

4）数据链路长，一旦底层链路出错，下游数据都会出错；另外进行数据恢复耗时长并且排查数据问题需要逐层检查

总结而言，还是好处大于坏处的，而且本身这样的分层最重要的是满足对数据需求的支撑

##### 3 DWD为什么要抽取出一个公共？

1）上游具有公共属性才能让下游重复利用

2）另外公共性的约束也能一定层度上保证数据的唯一性和一致性保证

dwd 与 dw 层本身都带有公共层的属性，这样做的好处在于表示数据的可重复利用性，另外在进行模型设计时，也应该考虑字段的公共性，不能将那些针对特定需求的逻辑字段沉淀到公共层

##### 4 你有建立过维表吗？

有的

##### 5 为什么要建立维表？

1）维表是数据分析中必要的数据，它是描述事件度量的环境，没有这种描述，那么度量将毫无意义，也没有分析的切入点

2）如果只建立包含维度和事实的融合宽表，那么数据表的粒度将难以保证，对数据表的理解难度也会增加

##### 6 怎么保证DWD表中的维度统一？

1）针对杂项维度的统一，可以将杂项维度抽取单独维护以进行统一

2）针对事实表中的其它维度属性，应该保证字段命名、类型统一

3）针对dwd层的维表，不要重复建设（维护总线矩阵）；对于多数据源的维度应该进行合并（垂直、水平）

##### 7 怎么确认一个指标的口径？

1）对于业务描述口径：需要准确对应到业务流程中，并且与业务方确认对业务流程的理解和口径理解，然后通过业务RD确认数据来源、粒度、最终转化为业务数据口径

2）对于业务提供的数据口径：要保持怀疑态度，根据自己的数据探索和业务理解以及与业务产品和RD的确认，对口径进行校验，如果有问题，可以与需求方进行讨论统一（工作中确实经常存在需求方给的口径不对的情况）

##### 8 怎么保证口径唯一？

1）确保与需求方达成共识，这是首要前提

2）搭建业务内的指标字典来规范化指标（描述词 + 名词 + 量词 + 衍生词/派生词），确保数仓内部不会发生指标认知歧义

3）指标口径的输出应该统一对外暴露底层（dwd/dw事件域宽表）口径

4）推广自己的指标字典，来统一口径输出，做口径输出的主人，不做搬运工

##### 9 是怎么确定业务域的？

业务域的划分相对而言比较抽象的概念，既不是面向需求的主题域，也不是面向数据的数据域划分，而是对于业务涵盖内容的划分；通常是按照业务部门进行业务域的划分。

##### 10 业务过程怎么确定？

通常将业务流程中一个动作行为看做为一个业务过程，比如创建订单、提交信审等（同时，业务过程应该有对应的数据支撑）

1）业务和数据探索 => 业务过程与数据的映射

2）进行需求的拆解来确认需要模型建设的业务过程

##### 11 怎么解决数据倾斜？



##### 12 你们的数据量有多大?



##### 13 大表和大表join的时候怎么办?

1）存在数据倾斜时，按照数据倾斜进行处理

2）不存在数据倾斜时

- 考虑进行 reduce task 数量的配置，提高并行度；考虑开启 map agg 来减少 shuffle 或 SQL 中实现预聚合
- 考虑利用中间表对大表进行预计算后再 JOIN
- SPARK 中除了设置 partions 的个数外，还可以通过设置 executor 的内存、cpu核心数、最大exector个数
- 另外 spark 3 中还有自适应的 partition 策略

##### 14 用户行为表，字段为 userid ， date ，event，用户id，日期和事件，事件可能为注册，登录，点击，浏览，下线之类，表是一个全量表，要求输出日期，新用户数，次一日留存量，次二日留存量，次三日留存量...次七日留存量，比如20200101新注册100人，0102这一百里登录了80，0103这一百里登录了70,0104这一百里登录了60 ...则输出20200101 100 80 70 60 ...，后续留存只和注册日期相关，不需要连续登录，写出201910月一个月的每天新注册用户量及次七日内留存用户量

```sql
-- 1 1  
-- 2 1
with tmp as (
	select userid, event_date, is_sign_up,
        first_value(event_date) over(partition by userid order by event_date asc) as sign_up_date
    from (
        select userid, event_date, is_sign_up, 
            max(is_sign_up) over(partition by userid) as is_new_sign_up
        from ( -- 得到每个用户每天一条数据（要么为注册，要么为行为 => 登录）
            select userid, event_date, max(is_sign_up) as is_sign_up
            from (
                select 
                    userid, 
                    substr(`date`, 1, 8) as event_date, 
                    if(
                        event = '登录' or event = '浏览' or event = '点击' or event = '下线', 
                        0, 
                        1
                    ) as is_sign_up
                from user_action
                where substr(`date`, 1, 6) = '201910'
            ) as tmp
            group by userid, event_date
        ) as tmp
    ) as tmp
    where is_new_sign_up = 1
)
select sign_up_date, count(if(gap_days = 0, userid, null)) as new_cnt,
	count(if(gap_days = 7, userid, null)) as 7_days_save
from (
    select userid, gap_days, min(event_date) as sign_up_date
    from (
        select userid, is_sign_up, event_date, 
            datediff(date_parse(event_date, 'yyyyMMdd'), date_parse(sign_up_date, 'yyyyMMdd')) as gap_days
        from tmp
    ) as tmp
    group by userid, gap_days
) as tmp 
group by sign_up_date
```



## 探探

##### 1 自我介绍一下

##### 2 你了解什么大数据技术栈

##### 3 你了解kafka吗？

##### 4 和我说一下你知道的kafka

##### 5 你知道拉链表吗？

##### 6 拉链表的实现说一下

##### 7 你怎么解决缓慢变化维？

##### 8 如果每天的增量有120亿，要解决缓慢变化维，有什么思路吗？

天级别周期的拉链表，每天增量120亿，那么历史数据量、最新的全量表都超级大

1）如果 120亿有天级别的重复，类似流水数据，那么先进行去重操作 => 数据量处理

2）考虑冷热字段区分，将变化维度属性拆分为一个表进行处理 => 数据量处理

3）如果需要拉链（拉链 + 历史表 => 也就是保留一个全量最新数据分区）

.... ...... 拉链真不知道怎么优化诶，感觉要结合业务场景，做一些妥协好一点

可以考虑让需求方进行妥协 => 数据筛选、不做拉链（考虑直接覆盖或只保存前后状态）

##### 9 开窗函数

 

## 字节跳动

##### 1 自我介绍

##### 2 你经常使用什么语言开发？

##### 3 你对java还熟悉吗？

##### 4 Java和scala有什么不一样的地方?

##### 5 Spark的stage怎么划分？

按照宽窄依赖进行划分，出现宽依赖则会划分 stage

- 宽依赖：上游 RDD 中一个分区中的数据，需要分发到下游多个分区中
- 窄依赖：上游 RDD 中一个或多个分区中的数据，只会发送到下游一个分区中

##### 6 Hive的底层是什么？

##### 7 MapReduce的shuffle流程是什么样的?具体说明一下

1）MAP 函数执行完成后，对 key 按照 指定的reduce 个数或者动态计算的个数进行hash取模分区

2）写入分区缓冲区时，如果缓冲区达到阈值，进行一次快排后溢写到磁盘

3）分区数据完成后，溢写磁盘的数据需要进行 归并排序为一个文件到磁盘

4）reduce 任务获取数据，并进行归并排序（来自于多个 map 任务）

##### 8 MapReduce有排序吗？

有的，一般认为一个 MR 作业，有三次排序

##### 9 怎么理解combiner？

combiner 是实现 reducer的一个自定义类，作用在map端分区和排序后，可以进行局部汇总以减少 shuffle 数据量

##### 10 Shuffle为什么要排序？

shuffle 排序是因为按照key取模划分后到分区，这个过程到达分区后并不是有序的，需要将key进行排序以便于 reducer 进行数据的合并，并进行后续的计算，比如说按照 key group by 的过程，如果reduce 端收到的数据是有序的，那么可以顺序遍历数据进行汇总并方便溢写磁盘。当不排序时，reduce 的聚合运算，如果出现溢写磁盘，可能存在更新磁盘数据的操作

##### 11 Map的输出是什么？

map 的输出时按照 key 哈希取模分区的排好序的数据，输出到磁盘上，由reduce 进行读取

##### 12 Combiner的默认条件知道吗？什么情况下会触发combiner？

在不考虑自定义实现的 Combiner 的情况下，通常分为以下几个情况

1）如果分区时发生溢写磁盘，可能会触发 Combiner

2）如果设置了溢写磁盘文件数的参数，当进行合并时，可能会触发 Combiner

##### 13 了解算法吗？

##### 14 数据结构还记得一些吗？

##### 15 满树和完全树有什么区别？

- 满树指的是二叉树的每一层都是满的
- 完全二叉树指的是除了最后一层外，其它层都是满的，且最后一层是左靠齐的（可以用数组存储）

##### 16 二叉树的遍历了解吗？有哪些？都是怎么遍历的？

- 中序遍历，先遍历左节点然后当前节点，然后是右节点；在二叉搜索树中，排序后的结果是一个升序
- 前序遍历，先遍历当前节点，再遍历左节点和右节点
- 后续遍历，先遍历左右节点，再遍历当前节点

##### 17 单向链表了解吗？

##### 18 你们是怎么建模的？

##### 19 为什么要建模？建模有什么好处？

- 规范化，能够保证模型的质量，统一
- 简单化，更容易理解模型之间的关系和模型的含义
- 方便化，更容易获取到想要的数据，且维度与事实表的设计能支持复杂的分析查询
- 响应变化，响应数据、业务、需求变化的能力强

##### 20 数据倾斜的时候怎么解决？

##### 21 你解决过数据倾斜吗？

##### 22 SQL是怎么翻译成MapReduce的？

##### 23 你知道怎么快速的定位是哪段代码出问题了吗？

##### 24 有两张表，一张合同表，存放合同id ：conid，合同的价格 price和实际收取sj_price，另一张订单表 存放订单id ：orderid ，合同id ：conid 和订单的价格 ：price。合同表conid唯一，订单表中订单id唯一，多个订单对应一个合同。

1) 合同表数据

| Conid | price | sj_price |
| ----- | ----- | -------- |
| 001   | 700   | 500      |

2) 订单表数据

| Orderid | conid | price |
| ------- | ----- | ----- |
| 1       | 001   | 300   |
| 2       | 001   | 100   |
| 3       | 001   | 180   |
| 4       | 001   | 120   |

现在因为合同表的实际收取价格500比原本的价格700少，那么现在需要按相应的比例算出这个合同的每个订单在打折后是多少钱，要求每一个订单打折后的价格加起来等于合同打折后的价格（比如例子中的500） 【sql题】

##### 25 怎么判断一个链表有环？

> 解题论点

1）一个有环链表，两个指针同时从起点出发，A 指针每次访问当前节点的下一个节点，B 指针每次访问当前节点往后的第二个节点，则再经过 N 次访问后，会出现 A 指针与 B 指针访问的是同一个节点。

2）一个无环链表，一直访问下一个节点，会遇见 null 节点。

> 代码

```java
public static boolean isRing(ListNode head) {
    // 参数判断
    if(head == null) {
        return false;
    }
    ListNode oneStep = head;
    ListNode twoStep = head;

    while(true) {
        // 每次访问下一个节点
        oneStep = head.next;
        if(oneStep == null) {
            return false;
        }
        // 每次访问下下个节点
        if(twoStep.next == null || twoStep.next.next == null) {
            return false;
        }
        twoStep = twoStep.next.next;
        // 判断对象相等
        if(oneStep == twoStep) {
            return true;
        }
    }
}

// 暴力解法，引入 hash 表存储访问过的节点，算法时间复杂度近 O(n)
public static boolean isRingViolence(ListNode head) {

    if(head == null) {
        return false;
    }
    ListNode cur = head;
    // 引入 hash 存储遍历节点
    Map<ListNode, Character> saw = new HashMap<>();

    while (cur != null) {
        if(saw.get(cur) != null) {
            return true;
        }

        saw.put(cur, 'Y');
        cur = cur.next;
    }
    
    return false;
}
```

##### 26 模仿实现微信的随机红包

> 随机不夹杂金额的控制，直接随机

```java
public static double[] red(int count, double money) {

    if(count <= 0 || money <= 0) {
        return null;
    }
		// 两位小数
    DecimalFormat decimalFormat = new DecimalFormat("0.00");

    double[] rs = new double[count];

    for(int i = 1; i <= count; i++) {
        rs[i - 1] = Double.parseDouble(decimalFormat.format(get(count, i, money)));
        money -= rs[i - 1];
    }

    return rs;
}

// 根据剩余金额计算随机金额 = 随机数 * 剩余金额
private static double get(int allCnt, int count, double remain) {

    if(allCnt == count) {
        return remain;
    }

    return remain * random();
}

// 获取随机数 > 0.00 且 < 1
private static double random() {
    Random random = new Random();
    double ranm = random.nextDouble();
		// 随机数大于 0.00 判断
    if(ranm == 0.00) {
        return random();
    }
    return ranm;
}
// 测试
public static void main(String[] args) {
    Arrays.stream(red(4, 4)).forEach(dd -> System.out.println(dd));
}
```

> 二倍均值法

```java
// 测试
public static void main(String[] args) {
    Arrays.stream(redPacket(4, 0.02)).forEach(dd -> System.out.println(dd));
}

public static double[] redPacket(int count, double money) {
    if(count <= 0 || money <= 0.00) {
        return null;
    }

    double[] rs = new double[count];
    for(int i = 1; i <= count; i++) {
      	// 计算随机金额
        rs[i - 1] = randomPacket(count - i + 1, money);

        money -= rs[i - 1];
    }
    return rs;
}

private static double randomPacket(int remainCount, double remainMoney) {
	
    if(remainCount == 1) {
        return remainMoney;
    }
		// 二倍均值 = 剩余金额 / 剩余红包数量 * 2 * 随机数
    double money = remainMoney / remainCount * 2 * Math.random();
		// 返回三位小数
    return Math.floor(money * 1000) / 1000;
}
```

##### 27 如果一个单向链表有交叉，求出交叉的节点

> 同判断链表是否有环

```java
public static ListNode isRing(ListNode head) {
    // 参数判断
    if(head == null) {
        return null;
    }
    ListNode oneStep = head;
    ListNode twoStep = head;

    while(true) {
        // 每次访问下一个节点
        oneStep = head.next;
        if(oneStep == null) {
            return null;
        }
        // 每次访问下下个节点
        if(twoStep.next == null || twoStep.next.next == null) {
            return null;
        }
        twoStep = twoStep.next.next;
        // 判断对象相等
        if(oneStep == twoStep) {
            return oneStep;
        }
    }
}
```

